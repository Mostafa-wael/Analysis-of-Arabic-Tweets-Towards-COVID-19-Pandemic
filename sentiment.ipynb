{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/mostafawael/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mostafawael/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/mostafawael/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mostafawael/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#  For Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo \n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import missingno as msno\n",
    "from wordcloud import WordCloud\n",
    "import random\n",
    "\n",
    "# For models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# For NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import pickle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# For Styling\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Downloading periphrals\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from preprocessing import *\n",
    "from plot import *\n",
    "from feature_extractor import *\n",
    "from data_balance import *\n",
    "from model import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = 'out/models/stance/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size = (7988, 3)\n",
      "Dev dataset size = (2000, 4)\n"
     ]
    }
   ],
   "source": [
    "train_file = 'Dataset/trainDev.csv'\n",
    "devFile = 'Dataset/ourTest.csv'\n",
    "train_df = pd.read_csv(train_file)\n",
    "dev_df = pd.read_csv(devFile)\n",
    "print(f\"Training dataset size = {train_df.shape}\")\n",
    "print(f\"Dev dataset size = {dev_df.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Training dataset size = (7462, 3)\n",
      "Cleaned Dev dataset size = (2000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning\n",
    "training_data = cleanData(train_df, 'training', clean = True, clearData = True)\n",
    "print(f\"Cleaned Training dataset size = {training_data.shape}\")\n",
    "# Data cleaning\n",
    "dev_data = cleanData(dev_df, 'dev', clean = True, clearData = False)\n",
    "print(f\"Cleaned Dev dataset size = {dev_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Training dataset size = (7462, 5)\n",
      "Index(['text', 'category', 'stance', 'Lemmatization', 'sentiment'], dtype='object')\n",
      "Processed dev dataset size = (2000, 6)\n",
      "Index(['id', 'text', 'stance', 'predictions', 'Lemmatization', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Some preprocessing steps, like extracting limmitization\n",
    "training_data = training_data.pipe(processing)    \n",
    "print(f\"Processed Training dataset size = {training_data.shape}\")\n",
    "print(training_data.columns)\n",
    "# Some preprocessing steps, like extracting limmitization\n",
    "dev_data = processing(dev_data)   \n",
    "print(f\"Processed dev dataset size = {dev_data.shape}\")\n",
    "print(dev_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the out\n",
    "training_data.to_csv('out/training_data_processed.csv', index=False) # print the df in a csv file\n",
    "# Save the out\n",
    "dev_data.to_csv('out/dev_data_processed.csv', index=False) # print the df in a csv file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  embeddings\n",
    "w2v_model, bow_model, tfidf_model = get_feature_models(training_data['Lemmatization'].tolist()) #+dev_data['Lemmatization'].tolist()) # use word2vec to extract the word embeddings\n",
    "training_data['features'] = get_features(w2v_model, bow_model, tfidf_model, training_data['Lemmatization'], 1, 1, 1) # get the word embeddings for each tweet\n",
    "trainingFeatures = training_data['features'].to_numpy()\n",
    "\n",
    "##  embeddings\n",
    "#model = extractWordEmbeddings(dev_data['Lemmatization']) # use word2vec to extract the word embeddings\n",
    "dev_data['features'] = get_features(w2v_model, bow_model, tfidf_model, dev_data['Lemmatization'], 1, 1, 1) # get the word embeddings for each tweet\n",
    "devFeatures = dev_data['features'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "18272\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_data['features']))\n",
    "print(len(dev_data['features'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLM embeddings\n",
    "# features = np.load('out/train_embeddings.npy')\n",
    "# XLM embeddings\n",
    "# dev_data['features'] = np.load('out/test_embeddings.npy').tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing:\n",
      "Class=2, n=5943 (79.644%)\n",
      "Class=1, n=1063 (14.246%)\n",
      "Class=0, n=456 (6.111%)\n",
      "After balancing:\n",
      "Class=2, n=5943 (33.333%)\n",
      "Class=1, n=5943 (33.333%)\n",
      "Class=0, n=5943 (33.333%)\n",
      "Some notes about dimensions of the data\n",
      "X_train size before cleaning = (7462, 18272)\n",
      "X_train size = (17829, 18272)\n",
      "y_train size = (17829,)\n"
     ]
    }
   ],
   "source": [
    "# convert features into 2d array\n",
    "trainingFeatures = np.array([np.array(xi) for xi in trainingFeatures])\n",
    "stances = training_data['stance'].to_numpy()\n",
    "columns = [\"f\" + str(i + 1) for i in range(len(trainingFeatures[0]))]\n",
    "\n",
    "df = pd.DataFrame(trainingFeatures, columns=columns)\n",
    "df = pd.DataFrame(trainingFeatures)\n",
    "df['stance'] = stances\n",
    "\n",
    "X_train_balanced, y_train_balanced = balance_data(df)\n",
    "print(\"Some notes about dimensions of the data\")\n",
    "print(f\"X_train size before cleaning = {trainingFeatures.shape}\")\n",
    "print(f\"X_train size = {X_train_balanced.shape}\")\n",
    "print(f\"y_train size = {y_train_balanced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test size = (2000, 18272)\n",
      "y_test size = (2000,)\n",
      "Unique values in the dataset = [-1  0  1]\n",
      "Unique values in the dataset = [-1  0  1]\n"
     ]
    }
   ],
   "source": [
    "split = 'n'\n",
    "if split == 'y':\n",
    "    # Split the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train_balanced, y_train_balanced, test_size=0.5, random_state=42)\n",
    "else:\n",
    "    X_train, y_train = X_train_balanced, y_train_balanced\n",
    "    X_test = np.array([np.array(xi) for xi in dev_data['features'].to_numpy()])\n",
    "    # X_test = np.load('out/test_embeddings.npy')\n",
    "    y_test = dev_data['stance'].to_numpy()\n",
    "\n",
    "print(f\"X_test size = {X_test.shape}\")\n",
    "print(f\"y_test size = {y_test.shape}\")\n",
    "# print unique values in the dataset\n",
    "print(f\"Unique values in the dataset = {np.unique(y_test)}\")\n",
    "print(f\"Unique values in the dataset = {np.unique(y_train)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "**Build a multi-class classifier to predict the category of the tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "18272\n",
      "7462\n",
      "18272\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_data['features']))\n",
    "print(len(dev_data['features'][0]))\n",
    "print(len(training_data['features']))\n",
    "print(len(training_data['features'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.56      0.28      0.37       619\n",
      "           0       0.25      0.19      0.22       697\n",
      "           1       0.43      0.74      0.55       684\n",
      "\n",
      "    accuracy                           0.41      2000\n",
      "   macro avg       0.42      0.40      0.38      2000\n",
      "weighted avg       0.41      0.41      0.38      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4055"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=500, max_depth=15, random_state=1, n_jobs=6)\n",
    "model, report = modelPipeline(X_train, y_train, X_test, y_test, clf, MODELS_PATH+ 'clf.model')\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.12      0.22       619\n",
      "           1       0.23      0.10      0.14       697\n",
      "           2       0.39      0.92      0.55       684\n",
      "\n",
      "    accuracy                           0.39      2000\n",
      "   macro avg       0.47      0.38      0.30      2000\n",
      "weighted avg       0.46      0.39      0.30      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.388"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xgboost\n",
    "xgb = XGBClassifier()\n",
    "_y_train = y_train + 1\n",
    "_y_test = y_test + 1\n",
    "model, report = modelPipeline(X_train, _y_train, X_test, _y_test, xgb, MODELS_PATH + 'xgb.model')\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.31      0.71      0.43       619\n",
      "           0       0.19      0.09      0.12       697\n",
      "           1       0.36      0.12      0.19       684\n",
      "\n",
      "    accuracy                           0.29      2000\n",
      "   macro avg       0.29      0.31      0.25      2000\n",
      "weighted avg       0.29      0.29      0.24      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2935"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "gnb = GaussianNB(var_smoothing=10)\n",
    "model, report = modelPipeline(X_train, y_train, X_test, y_test, gnb, MODELS_PATH + 'gnb.model')\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm = svm.SVC()\n",
    "model, report = modelPipeline(X_train, y_train, X_test, y_test, svm, MODELS_PATH + 'svm.model')\n",
    "report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto ML\n",
    "Check out the [Auto SKlearn](https://automl.github.io/auto-sklearn/master/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.classification\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.utils.multiclass import type_of_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2022-12-26 20:11:46,959:Client-AutoML(1):b3f45e36-8548-11ed-916f-4fa44fc3bf46] Capping the per_run_time_limit to 947.0 to have time for a least 2 models in each process.\n"
     ]
    }
   ],
   "source": [
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=1900, # overall time in seconds\n",
    "    per_run_time_limit=1300, # time per model in seconds\n",
    "    initial_configurations_via_metalearning=0,\n",
    "    ensemble_size=10,\n",
    "    n_jobs=8,\n",
    "    smac_scenario_args={\"runcount_limit\": 1},\n",
    ")\n",
    "automl.fit(X_train, y_train)\n",
    "y_pred = automl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto-sklearn results:\n",
      "  Dataset name: b3f45e36-8548-11ed-916f-4fa44fc3bf46\n",
      "  Metric: accuracy\n",
      "  Best validation score: 0.917335\n",
      "  Number of target algorithm runs: 1\n",
      "  Number of successful target algorithm runs: 1\n",
      "  Number of crashed target algorithm runs: 0\n",
      "  Number of target algorithms that exceeded the time limit: 0\n",
      "  Number of target algorithms that exceeded the memory limit: 0\n",
      "\n",
      "          rank  ensemble_weight           type      cost   duration\n",
      "model_id                                                           \n",
      "2            1              1.0  random_forest  0.082665  66.039781\n"
     ]
    }
   ],
   "source": [
    "print(automl.sprint_statistics())\n",
    "print(automl.leaderboard())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.29443428870259153\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score\", sklearn.metrics.f1_score(y_test, y_pred, average='macro')) #  get the Score of the final ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.376\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred)) #  get the Score of the final ensemble"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Trial  \n",
    "###### it's bad one!\n",
    "### layers need modifications!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class NLPDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, x, y):\n",
    "    self.x = torch.tensor(x)\n",
    "    self.y = torch.tensor(y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.x.shape[0]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.x[idx], self.y[idx]\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "  def __init__(self, embedding_dim=100, hidden_size=100,num_layer= 3 , n_classes=3):\n",
    "\n",
    "    super(LSTM, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layer = num_layer\n",
    "    self.n_classes = n_classes\n",
    "    self.embedding_dim = embedding_dim\n",
    "\n",
    "    self.lstm = nn.LSTM(input_size =embedding_dim, hidden_size = hidden_size,num_layers = num_layer ,batch_first=True)\n",
    "    self.linear = nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "  def forward(self, emmbeddings):\n",
    "\n",
    "    lstm_out, state = self.lstm(emmbeddings)\n",
    "    final_output = self.linear(lstm_out)\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NLPDataset(X_train.tolist(), (y_train+1).tolist())\n",
    "dev_dataset = NLPDataset(X_test.tolist(), (y_test+1).tolist())\n",
    "model = LSTM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, batch_size=500, epochs=100, learning_rate=0.01):\n",
    "\n",
    "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  # GPU configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "  for epoch_num in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for train_input, train_label in (train_dataloader):\n",
    "\n",
    "      train_input, train_label = train_input.to(device), train_label.to(device)\n",
    "\n",
    "      output = model(train_input)\n",
    "      \n",
    "      batch_loss = criterion(output.view(-1, output.shape[-1]), train_label.view(-1))\n",
    "\n",
    "      total_loss_train += batch_loss\n",
    "\n",
    "      acc = (output.argmax(dim=-1) == train_label).sum().item()\n",
    "      total_acc_train += acc\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      batch_loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      \n",
    "    epoch_loss = total_loss_train / (len(train_dataset))\n",
    "\n",
    "    epoch_acc = total_acc_train / (len(train_dataset))\n",
    "    if (epoch_num+1) % 10 == 0:\n",
    "      print(\n",
    "          f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "          | Train Accuracy: {epoch_acc}\\n')\n",
    "\n",
    "def evaluate(model, test_dataset):\n",
    "\n",
    "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "  # GPU Configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "  total_acc_test = 0\n",
    "  \n",
    "  with torch.no_grad():\n",
    "\n",
    "      test_input, test_label = next(iter(test_dataloader))\n",
    "\n",
    "      test_input = test_input.to(device)\n",
    "      test_label = test_label.to(device)\n",
    "\n",
    "      output = model(test_input)\n",
    "\n",
    "      # classification report\n",
    "      report = (classification_report(test_label.cpu(), output.argmax(dim=-1).cpu()))\n",
    "      print(report)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes a while to train especially if you dont have a GPU \n",
    "# Advice: if you dont have a GPU, dont run this cell\n",
    "HAVE_A_GPU = False\n",
    "if HAVE_A_GPU:\n",
    "    train(model, train_dataset, batch_size=500, epochs=500, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        68\n",
      "           1       0.00      0.00      0.00       121\n",
      "           2       0.81      1.00      0.89       790\n",
      "\n",
      "    accuracy                           0.81       979\n",
      "   macro avg       0.27      0.33      0.30       979\n",
      "weighted avg       0.65      0.81      0.72       979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, dev_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
