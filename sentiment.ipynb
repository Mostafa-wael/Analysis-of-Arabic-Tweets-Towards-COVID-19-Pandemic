{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/robert/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/robert/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/robert/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/robert/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#  For Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo \n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import missingno as msno\n",
    "from wordcloud import WordCloud\n",
    "import random\n",
    "\n",
    "# For models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# For NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import pickle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# For Styling\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Downloading periphrals\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from preprocessing import *\n",
    "from plot import *\n",
    "from feature_extractor import *\n",
    "from data_balance import *\n",
    "from model import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size = (6988, 3)\n",
      "Dev dataset size = (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "train_file = 'Dataset/train.csv'\n",
    "devFile = 'Dataset/dev.csv'\n",
    "train_df = pd.read_csv(train_file)\n",
    "dev_df = pd.read_csv(devFile)\n",
    "print(f\"Training dataset size = {train_df.shape}\")\n",
    "print(f\"Dev dataset size = {dev_df.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Training dataset size = (6557, 3)\n",
      "Cleaned Dev dataset size = (979, 3)\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning\n",
    "training_data = cleanData(train_df, 'training', clean = False, clearData = True)\n",
    "print(f\"Cleaned Training dataset size = {training_data.shape}\")\n",
    "# Data cleaning\n",
    "dev_data = cleanData(dev_df, 'dev', clean = False, clearData = True)\n",
    "print(f\"Cleaned Dev dataset size = {dev_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Training dataset size = (6557, 5)\n",
      "Index(['text', 'category', 'stance', 'Lemmatization', 'sentiment'], dtype='object')\n",
      "Processed dev dataset size = (979, 5)\n",
      "Index(['text', 'category', 'stance', 'Lemmatization', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Some preprocessing steps, like extracting limmitization\n",
    "training_data = training_data.pipe(processing)    \n",
    "print(f\"Processed Training dataset size = {training_data.shape}\")\n",
    "print(training_data.columns)\n",
    "# Some preprocessing steps, like extracting limmitization\n",
    "dev_data = processing(dev_data)   \n",
    "print(f\"Processed dev dataset size = {dev_data.shape}\")\n",
    "print(dev_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the out\n",
    "training_data.to_csv('out/training_data_processed.csv', index=False) # print the df in a csv file\n",
    "# Save the out\n",
    "dev_data.to_csv('out/dev_data_processed.csv', index=False) # print the df in a csv file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word2Vec embeddings\n",
    "model = extractWordEmbeddings(training_data['Lemmatization'].tolist()+dev_data['Lemmatization'].tolist()) # use word2vec to extract the word embeddings\n",
    "training_data['features'] = getTweetsEmbeddings(model, training_data['Lemmatization']) # get the word embeddings for each tweet\n",
    "trainingFeatures = training_data['features'].to_numpy()\n",
    "\n",
    "## Word2Vec embeddings\n",
    "#model = extractWordEmbeddings(dev_data['Lemmatization']) # use word2vec to extract the word embeddings\n",
    "dev_data['features'] = getTweetsEmbeddings(model, dev_data['Lemmatization']) # get the word embeddings for each tweet\n",
    "devFeatures = dev_data['features'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLM embeddings\n",
    "# features = np.load('out/train_embeddings.npy')\n",
    "# XLM embeddings\n",
    "# dev_data['features'] = np.load('out/test_embeddings.npy').tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing:\n",
      "Class=2, n=5207 (79.411%)\n",
      "Class=1, n=954 (14.549%)\n",
      "Class=0, n=396 (6.039%)\n",
      "After balancing:\n",
      "Class=2, n=5207 (33.333%)\n",
      "Class=1, n=5207 (33.333%)\n",
      "Class=0, n=5207 (33.333%)\n",
      "Some notes about dimensions of the data\n",
      "X_train size before cleaning = (6557, 100)\n",
      "X_train size = (15621, 100)\n",
      "y_train size = (15621,)\n"
     ]
    }
   ],
   "source": [
    "# convert features into 2d array\n",
    "trainingFeatures = np.array([np.array(xi) for xi in trainingFeatures])\n",
    "stances = training_data['stance'].to_numpy()\n",
    "columns = [\"f\" + str(i + 1) for i in range(len(trainingFeatures[0]))]\n",
    "\n",
    "df = pd.DataFrame(trainingFeatures, columns=columns)\n",
    "df = pd.DataFrame(trainingFeatures)\n",
    "df['stance'] = stances\n",
    "\n",
    "X_train_balanced, y_train_balanced = balance_data(df)\n",
    "print(\"Some notes about dimensions of the data\")\n",
    "print(f\"X_train size before cleaning = {trainingFeatures.shape}\")\n",
    "print(f\"X_train size = {X_train_balanced.shape}\")\n",
    "print(f\"y_train size = {y_train_balanced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test size = (979, 100)\n",
      "y_test size = (979,)\n",
      "Unique values in the dataset = [-1  0  1]\n",
      "Unique values in the dataset = [-1  0  1]\n"
     ]
    }
   ],
   "source": [
    "split = 'n'\n",
    "if split == 'y':\n",
    "    # Split the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train_balanced, y_train_balanced, test_size=0.5, random_state=42)\n",
    "else:\n",
    "    X_train, y_train = X_train_balanced, y_train_balanced\n",
    "    X_test = np.array([np.array(xi) for xi in dev_data['features'].to_numpy()])\n",
    "    # X_test = np.load('out/test_embeddings.npy')\n",
    "    y_test = dev_data['stance'].to_numpy()\n",
    "\n",
    "print(f\"X_test size = {X_test.shape}\")\n",
    "print(f\"y_test size = {y_test.shape}\")\n",
    "# print unique values in the dataset\n",
    "print(f\"Unique values in the dataset = {np.unique(y_test)}\")\n",
    "print(f\"Unique values in the dataset = {np.unique(y_train)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "**Build a multi-class classifier to predict the category of the tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.20      0.34      0.25        68\n",
      "           0       0.36      0.54      0.43       121\n",
      "           1       0.90      0.78      0.84       790\n",
      "\n",
      "    accuracy                           0.72       979\n",
      "   macro avg       0.49      0.55      0.51       979\n",
      "weighted avg       0.79      0.72      0.75       979\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7201225740551583"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=500, max_depth=15, random_state=0)\n",
    "model, report = modelPipeline(X_train, y_train, X_test, y_test, clf, 'out/models/clf.model')\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.29      0.26        68\n",
      "           1       0.34      0.46      0.39       121\n",
      "           2       0.90      0.82      0.86       790\n",
      "\n",
      "    accuracy                           0.74       979\n",
      "   macro avg       0.49      0.53      0.50       979\n",
      "weighted avg       0.78      0.74      0.76       979\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7425944841675178"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xgboost\n",
    "xgb = XGBClassifier()\n",
    "_y_train = y_train + 1\n",
    "_y_test = y_test + 1\n",
    "model, report = modelPipeline(X_train, _y_train, X_test, _y_test, xgb, 'out/models/xgb.model')\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.08      0.66      0.15        68\n",
      "           0       0.31      0.46      0.37       121\n",
      "           1       0.95      0.31      0.46       790\n",
      "\n",
      "    accuracy                           0.35       979\n",
      "   macro avg       0.44      0.48      0.33       979\n",
      "weighted avg       0.81      0.35      0.43       979\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34933605720122574"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "gnb = GaussianNB(var_smoothing=10)\n",
    "model, report = modelPipeline(X_train, y_train, X_test, y_test, gnb, 'out/models/gnb.model')\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.12      0.51      0.19        68\n",
      "           0       0.29      0.51      0.37       121\n",
      "           1       0.95      0.57      0.71       790\n",
      "\n",
      "    accuracy                           0.56       979\n",
      "   macro avg       0.45      0.53      0.43       979\n",
      "weighted avg       0.81      0.56      0.64       979\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5597548518896833"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "svm = svm.SVC()\n",
    "model, report = modelPipeline(X_train, y_train, X_test, y_test, svm, 'out/models/svm.model')\n",
    "report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto ML\n",
    "Check out the [Auto SKlearn](https://automl.github.io/auto-sklearn/master/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.classification\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.utils.multiclass import type_of_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2022-12-26 04:57:48,991:Client-AutoML(1):12b61a65-84c9-11ed-bd65-3fd8cc76a64f] Capping the per_run_time_limit to 949.0 to have time for a least 2 models in each process.\n"
     ]
    }
   ],
   "source": [
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=1900, # overall time in seconds\n",
    "    per_run_time_limit=1300, # time per model in seconds\n",
    "    initial_configurations_via_metalearning=0,\n",
    "    ensemble_size=10,\n",
    "    n_jobs=8,\n",
    "    smac_scenario_args={\"runcount_limit\": 1},\n",
    ")\n",
    "automl.fit(X_train, y_train)\n",
    "y_pred = automl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto-sklearn results:\n",
      "  Dataset name: 12b61a65-84c9-11ed-bd65-3fd8cc76a64f\n",
      "  Metric: accuracy\n",
      "  Best validation score: 0.904947\n",
      "  Number of target algorithm runs: 1\n",
      "  Number of successful target algorithm runs: 1\n",
      "  Number of crashed target algorithm runs: 0\n",
      "  Number of target algorithms that exceeded the time limit: 0\n",
      "  Number of target algorithms that exceeded the memory limit: 0\n",
      "\n",
      "          rank  ensemble_weight           type      cost   duration\n",
      "model_id                                                           \n",
      "2            1              1.0  random_forest  0.095053  50.582338\n"
     ]
    }
   ],
   "source": [
    "print(automl.sprint_statistics())\n",
    "print(automl.leaderboard())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.48782095768694655\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score\", sklearn.metrics.f1_score(y_test, y_pred, average='macro')) #  get the Score of the final ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.7088866189989785\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred)) #  get the Score of the final ensemble"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Trial  \n",
    "###### it's bad one!\n",
    "### layers need modifications!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class NLPDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, x, y):\n",
    "    self.x = torch.tensor(x)\n",
    "    self.y = torch.tensor(y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.x.shape[0]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.x[idx], self.y[idx]\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "  def __init__(self, embedding_dim=100, hidden_size=100,num_layer= 3 , n_classes=3):\n",
    "\n",
    "    super(LSTM, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layer = num_layer\n",
    "    self.n_classes = n_classes\n",
    "    self.embedding_dim = embedding_dim\n",
    "\n",
    "    self.lstm = nn.LSTM(input_size =embedding_dim, hidden_size = hidden_size,num_layers = num_layer ,batch_first=True)\n",
    "    self.linear = nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "  def forward(self, emmbeddings):\n",
    "\n",
    "    lstm_out, state = self.lstm(emmbeddings)\n",
    "    final_output = self.linear(lstm_out)\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(100, 100, num_layers=3, batch_first=True)\n",
      "  (linear): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NLPDataset(X_train.tolist(), (y_train+1).tolist())\n",
    "dev_dataset = NLPDataset(X_test.tolist(), (y_test+1).tolist())\n",
    "model = LSTM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, batch_size=500, epochs=100, learning_rate=0.01):\n",
    "\n",
    "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  # GPU configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "  for epoch_num in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for train_input, train_label in (train_dataloader):\n",
    "\n",
    "      train_input, train_label = train_input.to(device), train_label.to(device)\n",
    "\n",
    "      output = model(train_input)\n",
    "      \n",
    "      batch_loss = criterion(output.view(-1, output.shape[-1]), train_label.view(-1))\n",
    "\n",
    "      total_loss_train += batch_loss\n",
    "\n",
    "      acc = (output.argmax(dim=-1) == train_label).sum().item()\n",
    "      total_acc_train += acc\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      batch_loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      \n",
    "    epoch_loss = total_loss_train / (len(train_dataset))\n",
    "\n",
    "    epoch_acc = total_acc_train / (len(train_dataset))\n",
    "    if (epoch_num+1) % 10 == 0:\n",
    "      print(\n",
    "          f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "          | Train Accuracy: {epoch_acc}\\n')\n",
    "\n",
    "def evaluate(model, test_dataset):\n",
    "\n",
    "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "  # GPU Configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "  total_acc_test = 0\n",
    "  \n",
    "  with torch.no_grad():\n",
    "\n",
    "      test_input, test_label = next(iter(test_dataloader))\n",
    "\n",
    "      test_input = test_input.to(device)\n",
    "      test_label = test_label.to(device)\n",
    "\n",
    "      output = model(test_input)\n",
    "\n",
    "      # classification report\n",
    "      report = (classification_report(test_label.cpu(), output.argmax(dim=-1).cpu()))\n",
    "      print(report)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss: 0.0017871001036837697           | Train Accuracy: 0.5895269188912362\n",
      "\n",
      "Epochs: 20 | Train Loss: 0.0017014176119118929           | Train Accuracy: 0.6112284744894693\n",
      "\n",
      "Epochs: 30 | Train Loss: 0.001687617041170597           | Train Accuracy: 0.6180142116381794\n",
      "\n",
      "Epochs: 40 | Train Loss: 0.001610406907275319           | Train Accuracy: 0.6409320786121248\n",
      "\n",
      "Epochs: 50 | Train Loss: 0.001586757367476821           | Train Accuracy: 0.6561039626144293\n",
      "\n",
      "Epochs: 60 | Train Loss: 0.0015397637616842985           | Train Accuracy: 0.6597528967415658\n",
      "\n",
      "Epochs: 70 | Train Loss: 0.0015123015036806464           | Train Accuracy: 0.6660265027847129\n",
      "\n",
      "Epochs: 80 | Train Loss: 0.0014970707707107067           | Train Accuracy: 0.6682670763715511\n",
      "\n",
      "Epochs: 90 | Train Loss: 0.0014487055595964193           | Train Accuracy: 0.6862556814544524\n",
      "\n",
      "Epochs: 100 | Train Loss: 0.001393694314174354           | Train Accuracy: 0.6996351065872863\n",
      "\n",
      "Epochs: 110 | Train Loss: 0.0013372763060033321           | Train Accuracy: 0.7076371551117085\n",
      "\n",
      "Epochs: 120 | Train Loss: 0.0012839807895943522           | Train Accuracy: 0.725497727418219\n",
      "\n",
      "Epochs: 130 | Train Loss: 0.0012426686007529497           | Train Accuracy: 0.7391332181038346\n",
      "\n",
      "Epochs: 140 | Train Loss: 0.0011538281105458736           | Train Accuracy: 0.7601305934319186\n",
      "\n",
      "Epochs: 150 | Train Loss: 0.0010849538957700133           | Train Accuracy: 0.7795915754433135\n",
      "\n",
      "Epochs: 160 | Train Loss: 0.0010062927613034844           | Train Accuracy: 0.7971960821970424\n",
      "\n",
      "Epochs: 170 | Train Loss: 0.0009723060065880418           | Train Accuracy: 0.8041098521221433\n",
      "\n",
      "Epochs: 180 | Train Loss: 0.0009087763610295951           | Train Accuracy: 0.819473785289034\n",
      "\n",
      "Epochs: 190 | Train Loss: 0.0008510960615240037           | Train Accuracy: 0.8329812431982587\n",
      "\n",
      "Epochs: 200 | Train Loss: 0.0009410541970282793           | Train Accuracy: 0.8165290314320466\n",
      "\n",
      "Epochs: 210 | Train Loss: 0.0007504604291170835           | Train Accuracy: 0.8544267332437104\n",
      "\n",
      "Epochs: 220 | Train Loss: 0.0007216682424768806           | Train Accuracy: 0.8660136995070739\n",
      "\n",
      "Epochs: 230 | Train Loss: 0.0006504373159259558           | Train Accuracy: 0.8794571410281032\n",
      "\n",
      "Epochs: 240 | Train Loss: 0.0006069149821996689           | Train Accuracy: 0.8850905831892965\n",
      "\n",
      "Epochs: 250 | Train Loss: 0.0005654770648106933           | Train Accuracy: 0.8946290250304078\n",
      "\n",
      "Epochs: 260 | Train Loss: 0.00059239671099931           | Train Accuracy: 0.8909800909032712\n",
      "\n",
      "Epochs: 270 | Train Loss: 0.0005370923317968845           | Train Accuracy: 0.8994942705332565\n",
      "\n",
      "Epochs: 280 | Train Loss: 0.0005181002779863775           | Train Accuracy: 0.903975417706933\n",
      "\n",
      "Epochs: 290 | Train Loss: 0.0005182012682780623           | Train Accuracy: 0.905511811023622\n",
      "\n",
      "Epochs: 300 | Train Loss: 0.00040027903742156923           | Train Accuracy: 0.9269573010690737\n",
      "\n",
      "Epochs: 310 | Train Loss: 0.00044337910367175937           | Train Accuracy: 0.9188272197682607\n",
      "\n",
      "Epochs: 320 | Train Loss: 0.0003670328005682677           | Train Accuracy: 0.9335509890531977\n",
      "\n",
      "Epochs: 330 | Train Loss: 0.00038516405038535595           | Train Accuracy: 0.9300300877024519\n",
      "\n",
      "Epochs: 340 | Train Loss: 0.0003795631055254489           | Train Accuracy: 0.9298380385378657\n",
      "\n",
      "Epochs: 350 | Train Loss: 0.0004663463623728603           | Train Accuracy: 0.9158184495230779\n",
      "\n",
      "Epochs: 360 | Train Loss: 0.0003644509124569595           | Train Accuracy: 0.9352794315344728\n",
      "\n",
      "Epochs: 370 | Train Loss: 0.00035442321677692235           | Train Accuracy: 0.9373919723449203\n",
      "\n",
      "Epochs: 380 | Train Loss: 0.00035329212551005185           | Train Accuracy: 0.9380961526150695\n",
      "\n",
      "Epochs: 390 | Train Loss: 0.0003148601681459695           | Train Accuracy: 0.9466103322450548\n",
      "\n",
      "Epochs: 400 | Train Loss: 0.0002829251461662352           | Train Accuracy: 0.9517316433006849\n",
      "\n",
      "Epochs: 410 | Train Loss: 0.00030388712184503675           | Train Accuracy: 0.9472504961270085\n",
      "\n",
      "Epochs: 420 | Train Loss: 0.00023930509632918984           | Train Accuracy: 0.9587094296139812\n",
      "\n",
      "Epochs: 430 | Train Loss: 0.00026656841509975493           | Train Accuracy: 0.9518596760770758\n",
      "\n",
      "Epochs: 440 | Train Loss: 0.00025543427909724414           | Train Accuracy: 0.955636642980603\n",
      "\n",
      "Epochs: 450 | Train Loss: 0.0002449899911880493           | Train Accuracy: 0.9577491837910506\n",
      "\n",
      "Epochs: 460 | Train Loss: 0.0002448435116093606           | Train Accuracy: 0.9553805774278216\n",
      "\n",
      "Epochs: 470 | Train Loss: 0.00030149074154905975           | Train Accuracy: 0.946482299468664\n",
      "\n",
      "Epochs: 480 | Train Loss: 0.00021448853658512235           | Train Accuracy: 0.9629345112348762\n",
      "\n",
      "Epochs: 490 | Train Loss: 0.00020862971723545343           | Train Accuracy: 0.9648550028807374\n",
      "\n",
      "Epochs: 500 | Train Loss: 0.0002429827000014484           | Train Accuracy: 0.9590295115549581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# it takes a while to train especially if you dont have a GPU \n",
    "# Advice: if you dont have a GPU, dont run this cell\n",
    "train(model, train_dataset, batch_size=500, epochs=500, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.32      0.24        68\n",
      "           1       0.26      0.40      0.32       121\n",
      "           2       0.88      0.75      0.81       790\n",
      "\n",
      "    accuracy                           0.68       979\n",
      "   macro avg       0.44      0.49      0.46       979\n",
      "weighted avg       0.75      0.68      0.71       979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, dev_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdae8ebc0151ec65c4f1198b193fec26fb9b849a179ab75edca17b99fd707516"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
