{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/makrion/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/makrion/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For Data\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#  For Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo \n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import missingno as msno\n",
    "from wordcloud import WordCloud\n",
    "import random \n",
    "\n",
    "# For NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For Styling\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Downloading periphrals\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size = (6988, 3)\n",
      "Dev dataset size = (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "file = '../Dataset/train.csv'\n",
    "df = pd.read_csv(file)\n",
    "devFile = '../Dataset/dev.csv'\n",
    "dev_df = pd.read_csv(devFile)\n",
    "print(f\"Train dataset size = {df.shape}\")\n",
    "print(f\"Dev dataset size = {dev_df.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanTweets(df):\n",
    "    data = df.copy() # Copying the dataset\n",
    "    ##### Related to the tweets #####\n",
    "    # Remove twitter handlers\n",
    "    data.text = data.text.apply(lambda x:re.sub('@[^\\s]+','',x))\n",
    "    # Remove digits\n",
    "    data.text = data.text.apply(lambda x:re.sub(r'\\d+','',x))\n",
    "    # Remove all the (special characters, punctuations, and emojis)\n",
    "    data.text = data.text.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n",
    "    # Remove all english alphabets\n",
    "    data.text = data.text.apply(lambda x:re.sub(r'[a-zA-Z]', '', x))\n",
    "    # Substituting multiple spaces with single space\n",
    "    data.text = data.text.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n",
    "    # Remove all the empty spaces\n",
    "    data.text = data.text.apply(lambda x: x.strip())\n",
    "    # Remove all the stopwords\n",
    "    data.text = data.text.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords.words('arabic'))]))\n",
    "    ##### Related to the dataset ##### \n",
    "    # Remove all the empty rows\n",
    "    data = data[data.text != '']\n",
    "    # Removing the duplicated rows\n",
    "    data = data.drop_duplicates()\n",
    "    # Removing the duplicated tweets\n",
    "    data = data.drop_duplicates(subset=['text'])\n",
    "    # Removing the tweets with less than 10 characters\n",
    "    data = data[data.text.str.len() > 10]\n",
    "    # Removing the tweets with less than 4 words\n",
    "    data = data[data.text.str.split().str.len() > 3]\n",
    "    # Resetting the index, why? because we removed some rows\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset size = (6557, 3)\n"
     ]
    }
   ],
   "source": [
    "def cleanData(df, name, clean = False):  # If you want to clean the data, set it to True. Default is False to save time\n",
    "    if clean:\n",
    "        data  = CleanTweets(df)\n",
    "        data.to_csv('../out/'+name+'_cleaned_data.csv', index=False) # print the df in a csv file\n",
    "        data.head() # Displaying the dataset\n",
    "    else:\n",
    "        # Read the cleaned data\n",
    "        data = pd.read_csv('../out/'+name+'_cleaned_data.csv')\n",
    "    return data\n",
    "data = cleanData(df, 'train', clean = False)\n",
    "print(f\"Cleaned dataset size = {data.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More data preprocessing\n",
    "**Extracting any required fields from the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset size = (6557, 9)\n"
     ]
    }
   ],
   "source": [
    "def processing(data):\n",
    "    # Apply Lemmatization to the tweets\n",
    "    from nltk.stem.isri import ISRIStemmer # Arabic Lemmatization\n",
    "    st = ISRIStemmer()\n",
    "    data['Lemmatization'] = data.text.apply(lambda x: ''.join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "    # Extract Sentiment Values for each tweet \n",
    "    data['sentiment'] = data['stance'].apply(lambda x: \n",
    "                                                'positive' if x == 1 \n",
    "                                                else ('negative' if x == -1 \n",
    "                                                else 'neutral' )); # Extracting the overall sentiment\n",
    "    # Useful Information\n",
    "    data['words'] = data.text.apply(lambda x:re.findall(r'\\w+', x ))\n",
    "    data['errors'] = data.words.apply(spell.unknown)\n",
    "    data['errorsCount'] = data.errors.apply(len)\n",
    "    data['sentenceLength'] = data.text.apply(len)\n",
    "\n",
    "    return data\n",
    "data = data.pipe(processing)    \n",
    "data.head() # show the dataset\n",
    "data.to_csv('../out/processed_data.csv', index=False) # print the df in a csv file\n",
    "print(f\"Processed dataset size = {data.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2022-12-25 22:56:21,175:gensim.models.word2vec] Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
     ]
    }
   ],
   "source": [
    "# Extract word embeddings for each tweet\n",
    "def extractWordEmbeddings(data):\n",
    "    from gensim.models import Word2Vec\n",
    "    model = Word2Vec(data, min_count=1, window=5, sg=0)\n",
    "    model.save('../out/word2vec.model')\n",
    "    return model\n",
    "model = extractWordEmbeddings(data['Lemmatization'])\n",
    "\n",
    "# use the model to extract word embeddings\n",
    "def getWordEmbeddings(model, word):\n",
    "    return model.wv[word]\n",
    "\n",
    "# get the word embeddings for each tweet\n",
    "def getTweetsEmbeddings(model, tweets):\n",
    "    return tweets.apply(lambda x: np.mean([getWordEmbeddings(model, word) for word in x], axis=0))\n",
    "\n",
    "# get the word embeddings for each tweet\n",
    "data['features'] = getTweetsEmbeddings(model, data['Lemmatization'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing:\n",
      "Class=1, n=897 (13.680%)\n",
      "Class=2, n=3422 (52.189%)\n",
      "Class=4, n=972 (14.824%)\n",
      "Class=9, n=293 (4.469%)\n",
      "Class=5, n=577 (8.800%)\n",
      "Class=6, n=88 (1.342%)\n",
      "Class=3, n=150 (2.288%)\n",
      "Class=8, n=75 (1.144%)\n",
      "Class=0, n=67 (1.022%)\n",
      "Class=7, n=16 (0.244%)\n",
      "After balancing:\n",
      "Class=1, n=3422 (10.000%)\n",
      "Class=2, n=3422 (10.000%)\n",
      "Class=4, n=3422 (10.000%)\n",
      "Class=9, n=3422 (10.000%)\n",
      "Class=5, n=3422 (10.000%)\n",
      "Class=6, n=3422 (10.000%)\n",
      "Class=3, n=3422 (10.000%)\n",
      "Class=8, n=3422 (10.000%)\n",
      "Class=0, n=3422 (10.000%)\n",
      "Class=7, n=3422 (10.000%)\n",
      "Some notes about dimensions of the data\n",
      "X_train size before cleaning = (6557, 100)\n",
      "X_train size = (34220, 100)\n",
      "y_train size = (34220,)\n"
     ]
    }
   ],
   "source": [
    "from data_balance import *\n",
    "\n",
    "# convert features into 2d array\n",
    "trainingFeatures = np.array([np.array(xi) for xi in data['features']])\n",
    "stances = data['stance'].to_numpy()\n",
    "columns = [\"f\" + str(i + 1) for i in range(len(trainingFeatures[0]))]\n",
    "\n",
    "df = pd.DataFrame(trainingFeatures, columns=columns)\n",
    "df = pd.DataFrame(trainingFeatures)\n",
    "df['stance'] = stances\n",
    "\n",
    "X, y = balance_data(df)\n",
    "print(\"Some notes about dimensions of the data\")\n",
    "print(f\"X_train size before cleaning = {trainingFeatures.shape}\")\n",
    "print(f\"X_train size = {X.shape}\")\n",
    "print(f\"y_train size = {y.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures():\n",
    "    return np.load('../out/balanced_data_stances/features.npy', allow_pickle=True)\n",
    "\n",
    "def getStances():\n",
    "    return np.load('../out/balanced_data_stances/stances.npy', allow_pickle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_data = cleanData(dev_df, 'dev', clean =False)\n",
    "# dev_data = processing(dev_data)   \n",
    "# model = extractWordEmbeddings(dev_data['Lemmatization'])\n",
    "# dev_data['features'] = getTweetsEmbeddings(model, dev_data['Lemmatization'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (17110, 100)\n",
      "X_test shape: (17110, 100)\n"
     ]
    }
   ],
   "source": [
    "# X = getFeatures()\n",
    "# y = getStances()\n",
    "\n",
    "# Split the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "**Build a multi-class classifier to predict the category of the tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(X_train, y_train, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "def testModel(X_test, y_test, model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "def evaluateModel(y_test, y_pred):\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return classification_report(y_test, y_pred, output_dict=True)['accuracy']\n",
    "def saveModel(model, model_name):\n",
    "    import pickle\n",
    "    pickle.dump(model, open(model_name, 'wb'))\n",
    "    return model_name\n",
    "def loadModel(model_name):\n",
    "    import pickle\n",
    "    return pickle.load(open(model_name, 'rb'))\n",
    "\n",
    "def modelPipeline(X_train, y_train, X_test, y_test, model, model_name):\n",
    "    model = trainModel(X_train, y_train, model)\n",
    "    y_pred = testModel(X_test, y_test, model)\n",
    "    report = evaluateModel(y_test, y_pred)\n",
    "    saveModel(model, model_name)\n",
    "    return model, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.98      0.99      0.99      1729\n",
      "           0       0.86      0.86      0.86      1690\n",
      "           1       0.80      0.43      0.56      1754\n",
      "           2       0.96      0.98      0.97      1712\n",
      "           3       0.74      0.87      0.80      1637\n",
      "           4       0.80      0.91      0.85      1720\n",
      "           5       0.96      1.00      0.98      1680\n",
      "           6       0.99      1.00      0.99      1722\n",
      "           7       0.96      0.99      0.98      1734\n",
      "           8       0.90      0.96      0.93      1732\n",
      "\n",
      "    accuracy                           0.90     17110\n",
      "   macro avg       0.90      0.90      0.89     17110\n",
      "weighted avg       0.90      0.90      0.89     17110\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8981297486849795"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=500, max_depth=15, random_state=0)\n",
    "model, report = modelPipeline(X_train, y_train, X_test, y_test, clf, '../out/clf.model')\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Xgboost\n",
    "# xgb = XGBClassifier()\n",
    "# _y_train = y_train + 1\n",
    "# _y_test = y_test + 1\n",
    "# model, report = modelPipeline(X_train, _y_train, X_test, _y_test, xgb, '../out/xgb.model')\n",
    "# report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.37      0.91      0.53      2641\n",
      "           0       0.61      0.02      0.03      2571\n",
      "           1       0.57      0.27      0.37      2599\n",
      "\n",
      "    accuracy                           0.41      7811\n",
      "   macro avg       0.52      0.40      0.31      7811\n",
      "weighted avg       0.51      0.41      0.31      7811\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4051977979772116"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB(var_smoothing=10)\n",
    "model, report = modelPipeline(X_train, y_train, X_test, y_test, gnb, '../out/gnb.model')\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SVM\n",
    "# from sklearn import svm\n",
    "# svm = svm.SVC()\n",
    "# model, report = modelPipeline(X_train, y_train, X_test, y_test, svm, 'out/svm.model')\n",
    "# report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto ML\n",
    "Check out the [Auto SKlearn](https://automl.github.io/auto-sklearn/master/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "import autosklearn.classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2022-12-25 22:51:19,049:Client-AutoML(1):e12e6bfe-8495-11ed-9e22-8deeefa998df] Capping the per_run_time_limit to 949.0 to have time for a least 2 models in each process.\n"
     ]
    }
   ],
   "source": [
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=1900, # overall time in seconds\n",
    "    per_run_time_limit=1300, # time per model in seconds\n",
    "    initial_configurations_via_metalearning=0,\n",
    "    ensemble_size=10,\n",
    "    n_jobs=8,\n",
    "    smac_scenario_args={\"runcount_limit\": 1},\n",
    ")\n",
    "automl.fit(X_train, y_train)\n",
    "y_pred = automl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto-sklearn results:\n",
      "  Dataset name: e12e6bfe-8495-11ed-9e22-8deeefa998df\n",
      "  Metric: accuracy\n",
      "  Best validation score: 0.851047\n",
      "  Number of target algorithm runs: 1\n",
      "  Number of successful target algorithm runs: 1\n",
      "  Number of crashed target algorithm runs: 0\n",
      "  Number of target algorithms that exceeded the time limit: 0\n",
      "  Number of target algorithms that exceeded the memory limit: 0\n",
      "\n",
      "          rank  ensemble_weight           type      cost   duration\n",
      "model_id                                                           \n",
      "2            1              1.0  random_forest  0.148953  28.906683\n"
     ]
    }
   ],
   "source": [
    "print(automl.sprint_statistics())\n",
    "print(automl.leaderboard())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.8486749455895531\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred)) #  get the Score of the final ensemble\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "1. https://www.kaggle.com/code/wonduk/text-clustering-pca-eda-on-covid19-dataset\n",
    "2. https://www.kaggle.com/code/haefatim/pfizer-tweets-eda-sentiment-analysis#%23-Reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0d277ff71215bc2e00e65dbe1083bb095effc9738a74808cbcc90b6ef0d8026"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
