{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/mostafawael/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mostafawael/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For Data\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#  For Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo \n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import missingno as msno\n",
    "from wordcloud import WordCloud\n",
    "import random \n",
    "\n",
    "# For NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For Styling\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Downloading periphrals\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size = (6988, 3)\n",
      "Dev dataset size = (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "file = '../Dataset/train.csv'\n",
    "df = pd.read_csv(file)\n",
    "devFile = '../Dataset/dev.csv'\n",
    "dev_df = pd.read_csv(devFile)\n",
    "print(f\"Train dataset size = {df.shape}\")\n",
    "print(f\"Dev dataset size = {dev_df.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanTweets(df):\n",
    "    data = df.copy() # Copying the dataset\n",
    "    ##### Related to the tweets #####\n",
    "    # Remove twitter handlers\n",
    "    data.text = data.text.apply(lambda x:re.sub('@[^\\s]+','',x))\n",
    "    # Remove digits\n",
    "    data.text = data.text.apply(lambda x:re.sub(r'\\d+','',x))\n",
    "    # Remove all the (special characters, punctuations, and emojis)\n",
    "    data.text = data.text.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n",
    "    # Remove all english alphabets\n",
    "    data.text = data.text.apply(lambda x:re.sub(r'[a-zA-Z]', '', x))\n",
    "    # Substituting multiple spaces with single space\n",
    "    data.text = data.text.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n",
    "    # Remove all the empty spaces\n",
    "    data.text = data.text.apply(lambda x: x.strip())\n",
    "    # Remove all the stopwords\n",
    "    data.text = data.text.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords.words('arabic'))]))\n",
    "    ##### Related to the dataset ##### \n",
    "    # Remove all the empty rows\n",
    "    data = data[data.text != '']\n",
    "    # Removing the duplicated rows\n",
    "    data = data.drop_duplicates()\n",
    "    # Removing the duplicated tweets\n",
    "    data = data.drop_duplicates(subset=['text'])\n",
    "    # Removing the tweets with less than 10 characters\n",
    "    data = data[data.text.str.len() > 10]\n",
    "    # Removing the tweets with less than 4 words\n",
    "    data = data[data.text.str.split().str.len() > 3]\n",
    "    # Resetting the index, why? because we removed some rows\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset size = (6557, 3)\n"
     ]
    }
   ],
   "source": [
    "def cleanData(df, name, clean = False):  # If you want to clean the data, set it to True. Default is False to save time\n",
    "    if clean:\n",
    "        data  = CleanTweets(df)\n",
    "        data.to_csv('../out/'+name+'_cleaned_data.csv', index=False) # print the df in a csv file\n",
    "        data.head() # Displaying the dataset\n",
    "    else:\n",
    "        # Read the cleaned data\n",
    "        data = pd.read_csv('../out/'+name+'_cleaned_data.csv')\n",
    "    return data\n",
    "data = cleanData(df, 'train', clean = False)\n",
    "print(f\"Cleaned dataset size = {data.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More data preprocessing\n",
    "**Extracting any required fields from the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset size = (6557, 9)\n"
     ]
    }
   ],
   "source": [
    "def processing(data):\n",
    "    # Apply Lemmatization to the tweets\n",
    "    from nltk.stem.isri import ISRIStemmer # Arabic Lemmatization\n",
    "    st = ISRIStemmer()\n",
    "    data['Lemmatization'] = data.text.apply(lambda x: ''.join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "    # Extract Sentiment Values for each tweet \n",
    "    data['sentiment'] = data['stance'].apply(lambda x: \n",
    "                                                'positive' if x == 1 \n",
    "                                                else ('negative' if x == -1 \n",
    "                                                else 'neutral' )); # Extracting the overall sentiment\n",
    "    # Useful Information\n",
    "    data['words'] = data.text.apply(lambda x:re.findall(r'\\w+', x ))\n",
    "    data['errors'] = data.words.apply(spell.unknown)\n",
    "    data['errorsCount'] = data.errors.apply(len)\n",
    "    data['sentenceLength'] = data.text.apply(len)\n",
    "\n",
    "    return data\n",
    "data = data.pipe(processing)    \n",
    "data.head() # show the dataset\n",
    "data.to_csv('../out/processed_data.csv', index=False) # print the df in a csv file\n",
    "print(f\"Processed dataset size = {data.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word embeddings for each tweet\n",
    "def extractWordEmbeddings(data):\n",
    "    from gensim.models import Word2Vec\n",
    "    model = Word2Vec(data, min_count=1, window=5, sg=0)\n",
    "    model.save('../out/word2vec.model')\n",
    "    return model\n",
    "model = extractWordEmbeddings(data['Lemmatization'])\n",
    "\n",
    "# use the model to extract word embeddings\n",
    "def getWordEmbeddings(model, word):\n",
    "    return model.wv[word]\n",
    "\n",
    "# get the word embeddings for each tweet\n",
    "def getTweetsEmbeddings(model, tweets):\n",
    "    return tweets.apply(lambda x: np.mean([getWordEmbeddings(model, word) for word in x], axis=0))\n",
    "\n",
    "# get the word embeddings for each tweet\n",
    "data['features'] = getTweetsEmbeddings(model, data['Lemmatization'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing:\n",
      "Class=2, n=5207 (79.411%)\n",
      "Class=1, n=954 (14.549%)\n",
      "Class=0, n=396 (6.039%)\n",
      "After balancing:\n",
      "Class=2, n=5207 (33.333%)\n",
      "Class=1, n=5207 (33.333%)\n",
      "Class=0, n=5207 (33.333%)\n",
      "Some notes about dimensions of the data\n",
      "X_train size before cleaning = (6557, 100)\n",
      "X_train size = (15621, 100)\n",
      "y_train size = (15621,)\n"
     ]
    }
   ],
   "source": [
    "from data_balance import *\n",
    "\n",
    "# convert features into 2d array\n",
    "trainingFeatures = np.array([np.array(xi) for xi in data['features']])\n",
    "stances = data['stance'].to_numpy()\n",
    "columns = [\"f\" + str(i + 1) for i in range(len(trainingFeatures[0]))]\n",
    "\n",
    "df = pd.DataFrame(trainingFeatures, columns=columns)\n",
    "df = pd.DataFrame(trainingFeatures)\n",
    "df['stance'] = stances\n",
    "\n",
    "X, y = balance_data(df)\n",
    "print(\"Some notes about dimensions of the data\")\n",
    "print(f\"X_train size before cleaning = {trainingFeatures.shape}\")\n",
    "print(f\"X_train size = {X.shape}\")\n",
    "print(f\"y_train size = {y.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures():\n",
    "    return np.load('../out/balanced_data_stances/features.npy', allow_pickle=True)\n",
    "\n",
    "def getStances():\n",
    "    return np.load('../out/balanced_data_stances/stances.npy', allow_pickle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_data = cleanData(dev_df, 'dev', clean =False)\n",
    "# dev_data = processing(dev_data)   \n",
    "# model = extractWordEmbeddings(dev_data['Lemmatization'])\n",
    "# dev_data['features'] = getTweetsEmbeddings(model, dev_data['Lemmatization'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1562, 100)\n",
      "X_test shape: (14059, 100)\n"
     ]
    }
   ],
   "source": [
    "# X = getFeatures()\n",
    "# y = getStances()\n",
    "\n",
    "# Split the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "**Build a multi-class classifier to predict the category of the tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(X_train, y_train, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "def testModel(X_test, y_test, model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "def evaluateModel(y_test, y_pred):\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return classification_report(y_test, y_pred, output_dict=True)['accuracy']\n",
    "def saveModel(model, model_name):\n",
    "    import pickle\n",
    "    pickle.dump(model, open(model_name, 'wb'))\n",
    "    return model_name\n",
    "def loadModel(model_name):\n",
    "    import pickle\n",
    "    return pickle.load(open(model_name, 'rb'))\n",
    "\n",
    "def modelPipeline(X_train, y_train, X_test, y_test, model, model_name):\n",
    "    model = trainModel(X_train, y_train, model)\n",
    "    y_pred = testModel(X_test, y_test, model)\n",
    "    report = evaluateModel(y_test, y_pred)\n",
    "    saveModel(model, model_name)\n",
    "    return model, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.83      0.78      4685\n",
      "           0       0.66      0.64      0.65      4698\n",
      "           1       0.68      0.61      0.64      4676\n",
      "\n",
      "    accuracy                           0.70     14059\n",
      "   macro avg       0.69      0.70      0.69     14059\n",
      "weighted avg       0.69      0.70      0.69     14059\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6959243189416032"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=500, max_depth=15, random_state=0)\n",
    "model, report = modelPipeline(X_train, y_train, X_test, y_test, clf, '../out/clf.model')\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.83      0.78      4685\n",
      "           1       0.66      0.67      0.66      4698\n",
      "           2       0.69      0.60      0.64      4676\n",
      "\n",
      "    accuracy                           0.70     14059\n",
      "   macro avg       0.70      0.70      0.69     14059\n",
      "weighted avg       0.70      0.70      0.69     14059\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6973468952272566"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xgboost\n",
    "xgb = XGBClassifier()\n",
    "_y_train = y_train + 1\n",
    "_y_test = y_test + 1\n",
    "model, report = modelPipeline(X_train, _y_train, X_test, _y_test, xgb, '../out/xgb.model')\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.91      0.51       264\n",
      "           1       0.00      0.00      0.00       246\n",
      "           2       0.51      0.19      0.28       272\n",
      "\n",
      "    accuracy                           0.37       782\n",
      "   macro avg       0.29      0.37      0.26       782\n",
      "weighted avg       0.30      0.37      0.27       782\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37468030690537085"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB(var_smoothing=10)\n",
    "model, report = modelPipeline(X_train, y_train, X_test, y_test, gnb, '../out/gnb.model')\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.62      0.57       264\n",
      "           1       0.51      0.52      0.51       246\n",
      "           2       0.66      0.53      0.58       272\n",
      "\n",
      "    accuracy                           0.55       782\n",
      "   macro avg       0.56      0.55      0.55       782\n",
      "weighted avg       0.56      0.55      0.56       782\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5549872122762148"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn import svm\n",
    "svm = svm.SVC()\n",
    "model, report = modelPipeline(X_train, y_train, X_test, y_test, svm, 'out/svm.model')\n",
    "report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto ML\n",
    "Check out the [Auto SKlearn](https://automl.github.io/auto-sklearn/master/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "import autosklearn.classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2022-12-24 19:55:13,915:Client-AutoML(1):1db8f54a-83b4-11ed-a009-09f3292e7554] Capping the per_run_time_limit to 949.0 to have time for a least 2 models in each process.\n"
     ]
    }
   ],
   "source": [
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=1900, # overall time in seconds\n",
    "    per_run_time_limit=1300, # time per model in seconds\n",
    "    initial_configurations_via_metalearning=0,\n",
    "    ensemble_size=10,\n",
    "    n_jobs=8,\n",
    "    smac_scenario_args={\"runcount_limit\": 1},\n",
    ")\n",
    "automl.fit(X_train, y_train)\n",
    "y_pred = automl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto-sklearn results:\n",
      "  Dataset name: 1db8f54a-83b4-11ed-a009-09f3292e7554\n",
      "  Metric: accuracy\n",
      "  Best validation score: 0.918113\n",
      "  Number of target algorithm runs: 1\n",
      "  Number of successful target algorithm runs: 1\n",
      "  Number of crashed target algorithm runs: 0\n",
      "  Number of target algorithms that exceeded the time limit: 0\n",
      "  Number of target algorithms that exceeded the memory limit: 0\n",
      "\n",
      "          rank  ensemble_weight           type      cost   duration\n",
      "model_id                                                           \n",
      "2            1              1.0  random_forest  0.081887  52.255295\n"
     ]
    }
   ],
   "source": [
    "print(automl.sprint_statistics())\n",
    "print(automl.leaderboard())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9168797953964194\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred)) #  get the Score of the final ensemble\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "1. https://www.kaggle.com/code/wonduk/text-clustering-pca-eda-on-covid19-dataset\n",
    "2. https://www.kaggle.com/code/haefatim/pfizer-tweets-eda-sentiment-analysis#%23-Reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
