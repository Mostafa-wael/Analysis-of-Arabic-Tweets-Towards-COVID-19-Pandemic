{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# For models\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# For NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "import pickle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from preprocessing import *\n",
    "from plot import *\n",
    "# from feature_extractor import *\n",
    "from data_balance import *\n",
    "from model import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size = (6988, 3)\n",
      "Dev dataset size = (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "train_file = 'Dataset/train.csv'\n",
    "devFile = 'Dataset/dev.csv'\n",
    "train_df = pd.read_csv(train_file)\n",
    "dev_df = pd.read_csv(devFile)\n",
    "print(f\"Training dataset size = {train_df.shape}\")\n",
    "print(f\"Dev dataset size = {dev_df.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Training dataset size = (6557, 3)\n",
      "Cleaned Dev dataset size = (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning\n",
    "training_data = cleanData(train_df, 'training', clean = False, clearData = True)\n",
    "print(f\"Cleaned Training dataset size = {training_data.shape}\")\n",
    "# Data cleaning\n",
    "dev_data = cleanData(dev_df, 'dev', clean = False, clearData = False)\n",
    "print(f\"Cleaned Dev dataset size = {dev_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Training dataset size = (6557, 5)\n",
      "Index(['text', 'category', 'stance', 'Lemmatization', 'sentiment'], dtype='object')\n",
      "Processed dev dataset size = (1000, 5)\n",
      "Index(['text', 'category', 'stance', 'Lemmatization', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Some preprocessing steps, like extracting limmitization\n",
    "training_data = training_data.pipe(processing)    \n",
    "print(f\"Processed Training dataset size = {training_data.shape}\")\n",
    "print(training_data.columns)\n",
    "# Some preprocessing steps, like extracting limmitization\n",
    "dev_data = processing(dev_data)   \n",
    "print(f\"Processed dev dataset size = {dev_data.shape}\")\n",
    "print(dev_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size = (6557, 30)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(training_data.text)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = tokenizer.texts_to_sequences(training_data.text)\n",
    "max_length = 30\n",
    "X_train = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "\n",
    "print(f\"X_train size = {X_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = tokenizer.texts_to_sequences(dev_data.text)\n",
    "X_test = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "y_test = pd.get_dummies(dev_data['stance']).values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "from gensim.models import KeyedVectors\n",
    "w2v_embeddings_index = {}\n",
    "TOTAL_EMBEDDING_DIM = 100\n",
    "embeddings_file = 'embeddings/full_grams_sg_100_twitter.mdl'\n",
    "w2v_model = KeyedVectors.load(embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1476715 word vectors.\n"
     ]
    }
   ],
   "source": [
    "for word in w2v_model.wv.vocab:\n",
    "    w2v_embeddings_index[word] = w2v_model[word]\n",
    "print('Loaded %s word vectors.'% len(w2v_embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix shape: (27600, 100)\n"
     ]
    }
   ],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, TOTAL_EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = w2v_embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(\"Embedding Matrix shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train size = (6557,)\n"
     ]
    }
   ],
   "source": [
    "# Balance the dataset with respect to stances\n",
    "y_train = training_data['stance'].to_numpy()\n",
    "# y_train = LabelEncoder().fit_transform(y_train)\n",
    "\n",
    "# # transform the dataset\n",
    "# oversample = SMOTE()\n",
    "# X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "# y_train = y_train - 1\n",
    "\n",
    "# # summarize distribution\n",
    "# counter = Counter(y_train)\n",
    "# print(\"After balancing:\")\n",
    "# for k,v in counter.items():\n",
    "#     per = v / len(y_train) * 100\n",
    "#     print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
    "\n",
    "# print(f\"X_train size = {X_train.shape}\")\n",
    "print(f\"y_train size = {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test size =  (6557, 3)\n"
     ]
    }
   ],
   "source": [
    "y_train = pd.get_dummies(y_train).values\n",
    "print('y_test size = ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5207\n",
      "954\n",
      "396\n"
     ]
    }
   ],
   "source": [
    "print(np.sum([y_train[i][2] == 1 for i in range(len(y_train))]))\n",
    "print(np.sum([y_train[i][1] == 1 for i in range(len(y_train))]))\n",
    "print(np.sum([y_train[i][0] == 1 for i in range(len(y_train))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test size = (1000, 30)\n",
      "y_test size = (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_test size = {X_test.shape}\")\n",
    "print(f\"y_test size = {y_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "369/369 [==============================] - 37s 94ms/step - loss: 0.5724 - accuracy: 0.7894 - val_loss: 0.5103 - val_accuracy: 0.7988\n",
      "Epoch 2/8\n",
      "369/369 [==============================] - 33s 88ms/step - loss: 0.4491 - accuracy: 0.8158 - val_loss: 0.5112 - val_accuracy: 0.7973\n",
      "Epoch 3/8\n",
      "369/369 [==============================] - 33s 90ms/step - loss: 0.3417 - accuracy: 0.8622 - val_loss: 0.5391 - val_accuracy: 0.7927\n",
      "Epoch 4/8\n",
      "369/369 [==============================] - 33s 90ms/step - loss: 0.2442 - accuracy: 0.9104 - val_loss: 0.6315 - val_accuracy: 0.8064\n",
      "Epoch 5/8\n",
      "369/369 [==============================] - 33s 89ms/step - loss: 0.1709 - accuracy: 0.9417 - val_loss: 0.7636 - val_accuracy: 0.7942\n",
      "Epoch 6/8\n",
      "369/369 [==============================] - 33s 88ms/step - loss: 0.1156 - accuracy: 0.9605 - val_loss: 0.8676 - val_accuracy: 0.8034\n",
      "Epoch 7/8\n",
      "369/369 [==============================] - 31s 83ms/step - loss: 0.0825 - accuracy: 0.9753 - val_loss: 0.8280 - val_accuracy: 0.7988\n",
      "Epoch 8/8\n",
      "369/369 [==============================] - 33s 90ms/step - loss: 0.0630 - accuracy: 0.9807 - val_loss: 0.8372 - val_accuracy: 0.7957\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Embedding(vocab_size, TOTAL_EMBEDDING_DIM, input_length=X_train.shape[1], weights=[embedding_matrix]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 8\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 20ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.56      0.20      0.29        70\n",
      "           0       0.37      0.33      0.34       126\n",
      "           1       0.86      0.93      0.90       804\n",
      "\n",
      "    accuracy                           0.80      1000\n",
      "   macro avg       0.60      0.48      0.51      1000\n",
      "weighted avg       0.78      0.80      0.78      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1) - 1\n",
    "y_test = dev_data['stance'].to_numpy()\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43b010d9f7d982b0242266748a4ec71b1a1d9ef54f57a230cfc8a9c34419b385"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
